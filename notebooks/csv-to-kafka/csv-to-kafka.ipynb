{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize HDFS client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdfsPrefix = hdfs://namenode1:8020\n",
       "hadoopConf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "hdfs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1885073787_40, ugi=root (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1885073787_40, ugi=root (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "val hdfsPrefix = sys.env(\"HDFS_URL\")\n",
    "val hadoopConf = new Configuration()\n",
    "hadoopConf.set(\"fs.defaultFS\", sys.env(\"HDFS_URL\"))\n",
    "val hdfs = FileSystem.get(hadoopConf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Application Jar to HDFS so that workers can see it too\n",
      "Initializing Spark context...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rnd = scala.util.Random$@3ed733e9\n",
       "appJarPath = /app/csv-to-kafka.jar\n",
       "conf = org.apache.spark.SparkConf@6a6e5077\n",
       "spark = org.apache.spark.sql.SparkSession@7da32949\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@7da32949"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "println(\"Copying Application Jar to HDFS so that workers can see it too\")\n",
    "val rnd = scala.util.Random\n",
    "val appJarPath = \"/app/csv-to-kafka.jar\"\n",
    "hdfs.copyFromLocalFile(new Path(\"/app/app.jar\"), new Path(appJarPath))\n",
    "\n",
    "println(\"Initializing Spark context...\")\n",
    "val conf = new SparkConf()\n",
    "               .setAppName(\"CSV to Kafka\")\n",
    "               .set(\"spark.cores.max\", \"2\")\n",
    "               .set(\"spark.jars\", hdfsPrefix + appJarPath)\n",
    "val spark: SparkSession = SparkSession.builder.config(conf).getOrCreate()\n",
    "//if you look in Spark Master UI, a application will be running after this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load partitioned CSV data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load partitioned CSV from HDFS\n",
      "Partitions loaded: 2\n",
      "+-------+--------------------+------------------+-------------------+----+\n",
      "|summary|            personId|               lat|                lon|type|\n",
      "+-------+--------------------+------------------+-------------------+----+\n",
      "|  count|                   4|                 4|                  4|   4|\n",
      "|   mean|                null|-5.966223979991914|  -49.1150839571495|null|\n",
      "| stddev|                null|3.1309343545864228| 18.583792823102733|null|\n",
      "|    min|f57ded61-bb16-4f5...|-8.824724236896516| -68.94285644612884| gps|\n",
      "|    max|fe398302-eace-4ed...|-3.157759723087311|-32.287343468170164| gps|\n",
      "+-------+--------------------+------------------+-------------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [time: timestamp, personId: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: timestamp, personId: string ... 3 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Load partitioned CSV from HDFS\")\n",
    "val df = spark.read\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(hdfsPrefix + \"/output/obfuscated-samples.csv\")\n",
    "\n",
    "println(\"Partitions loaded: \" + df.rdd.partitions.size)\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send data to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaRegistryAddr = http://schema-registry:8081\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "http://schema-registry:8081"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.avro._\n",
    "val schemaRegistryAddr = \"http://schema-registry:8081\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe locations to Kafka topic 'locations'\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"Writing dataframe locations to Kafka topic 'locations'\")\n",
    "df.selectExpr(\"CAST(personId AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "    .write\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\")\n",
    "    .option(\"topic\", \"locations\")\n",
    "    .save()\n",
    "println(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs.delete(appJarPath, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Spark session\n"
     ]
    }
   ],
   "source": [
    "println(\"Stop Spark session\")\n",
    "spark.stop()\n",
    "//if you look in Spark Master UI, no application will be running after stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Toree - Scala",
   "language": "scala",
   "name": "spark_-_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
