{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark context...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "conf = org.apache.spark.SparkConf@3311e290\n",
       "spark = org.apache.spark.sql.SparkSession@1384d805\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@1384d805"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "println(\"Initializing Spark context...\")\n",
    "val conf = new SparkConf().setAppName(\"Example App\")\n",
    "val spark: SparkSession = SparkSession.builder.config(conf).getOrCreate()\n",
    "//if you look in Spark Master UI, a application will be running after this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize HDFS client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdfsPrefix = hdfs://namenode1:8020\n",
       "hadoopConf = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\n",
       "hdfs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-240526695_40, ugi=root (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-240526695_40, ugi=root (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hdfsPrefix = sys.env(\"HDFS_URL\")\n",
    "val hadoopConf = new Configuration()\n",
    "hadoopConf.set(\"fs.defaultFS\", sys.env(\"HDFS_URL\"))\n",
    "val hdfs = FileSystem.get(hadoopConf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load real samples and obfuscate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying samples file to HDFS...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "srcPath = /notebooks/obfuscate-geo-samples/fake-samples.csv\n",
       "destPath = hdfs:/tmp/real-samples.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:/tmp/real-samples.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Copying samples file to HDFS...\")\n",
    "//NEVER SAVE REAL SAMPLES IN GIT!\n",
    "val srcPath = new Path(\"/notebooks/obfuscate-geo-samples/fake-samples.csv\")\n",
    "val destPath = new Path(\"hdfs:///tmp/real-samples.csv\")\n",
    "hdfs.copyFromLocalFile(srcPath, destPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CSV from HDFS to Dataframe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df = [time: timestamp, personId: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: timestamp, personId: string ... 3 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Load CSV from HDFS to Dataframe\")\n",
    "val df = spark.read\n",
    "          .format(\"com.databricks.spark.csv\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .load(hdfsPrefix + \"/tmp/real-samples.csv\")\n",
    "// df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create partitioned file by personId in HDFS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.util.NoSuchElementException\n",
       "Message: None.get\n",
       "StackTrace:   at scala.None$.get(Option.scala:347)\n",
       "  at scala.None$.get(Option.scala:345)\n",
       "  at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:173)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommand$class.metrics(DataWritingCommand.scala:51)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics$lzycompute(InsertIntoHadoopFsRelationCommand.scala:47)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.metrics(InsertIntoHadoopFsRelationCommand.scala:47)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics$lzycompute(commands.scala:100)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.metrics(commands.scala:100)\n",
       "  at org.apache.spark.sql.execution.SparkPlanInfo$.fromSparkPlan(SparkPlanInfo.scala:56)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Create partitioned file by personId in HDFS\")\n",
    "import org.apache.spark.HashPartitioner\n",
    "val df1 = df.repartition(50, $\"personId\")\n",
    "df1.write\n",
    "   .format(\"com.databricks.spark.csv\")\n",
    "   .mode(\"overwrite\")\n",
    "   .save(hdfsPrefix + \"/tmp/real-samples-partitioned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load partitioned CSV from HDFS\n",
      "Partitions loaded: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2 = [2019-04-18T03:44:31.000Z: timestamp, bbb: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2019-04-18T03:44:31.000Z: timestamp, bbb: string ... 3 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Load partitioned CSV from HDFS\")\n",
    "val df2 = spark.read\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(hdfsPrefix + \"/tmp/real-samples-partitioned.csv\")\n",
    "\n",
    "println(\"Partitions loaded: \" + df2.rdd.partitions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by user\n",
      "Create random ids and translate positions randomly for each user\n",
      "Flatten results\n",
      "Sort results by timestamp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd2 = ShuffledRDD[32] at groupBy at <console>:37\n",
       "rdd3 = MapPartitionsRDD[33] at map at <console>:40\n",
       "rdd4 = MapPartitionsRDD[34] at flatMap at <console>:53\n",
       "rdd5 = MapPartitionsRDD[39] at sortBy at <console>:59\n",
       "rdd6 = MapPartitionsRDD[40] at map at <console>:63\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[40] at map at <console>:63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.UUID.randomUUID\n",
    "\n",
    "println(\"Group by user\")\n",
    "val rdd2 = df2.rdd.groupBy(r => r(1))\n",
    "\n",
    "println(\"Create random ids and translate positions randomly for each user\")\n",
    "val rdd3 = rdd2.map(e => {\n",
    "    val rnd = scala.util.Random\n",
    "    val (k, v) = e\n",
    "    val idd = randomUUID().toString\n",
    "    val latd = rnd.nextFloat*0.1\n",
    "    val lond = rnd.nextFloat*0.1\n",
    "    val nm = v.map(a => {\n",
    "        (a(0),idd,a(2).asInstanceOf[Double]+latd,a(3).asInstanceOf[Double]+lond,a(4))\n",
    "    })\n",
    "    (k, nm)\n",
    "})\n",
    "\n",
    "println(\"Flatten results\")\n",
    "val rdd4 = rdd3.flatMap(e => {\n",
    "    val (k, v) = e\n",
    "    v\n",
    "})\n",
    "\n",
    "println(\"Sort results by timestamp\")\n",
    "val rdd5 = rdd4.sortBy[String](e => {\n",
    "    val (t, v, v2, v3, v4) = e\n",
    "    t.toString\n",
    "})\n",
    "val rdd6 = rdd5.map(e => {\n",
    "    (e._1.toString, e._2.toString, e._3.toString, e._4.toString, e._5.toString)\n",
    "})\n",
    "\n",
    "// rdd5.foreach(e => {\n",
    "//   println(\"#3333#\" + e)\n",
    "// })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save obfuscated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to HDFS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fp = hdfs://namenode1:8020/output/obfuscated-samples.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs://namenode1:8020/output/obfuscated-samples.csv"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Saving to HDFS\")\n",
    "val fp = hdfsPrefix + \"/output/obfuscated-samples.csv\"\n",
    "hdfs.delete(new org.apache.hadoop.fs.Path(fp), true)\n",
    "rdd6.saveAsTextFile(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save obfuscated file to workspace\n",
      "Writing CSV file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "obr = Array((2019-04-18 03:49:11.0,5535f25f-0b1d-40c2-a228-692363f9d9d1,-8.564107401733398,-31.26030477534485,gps), (2019-04-18 03:55:33.0,f9e65ed5-f4aa-4624-8e1f-25e3fa44b062,-22.093605798416135,-43.417351112451556,gps), (2019-04-18 03:57:52.0,2aa7fe75-4fb9-456e-b574-adab37f46996,-7.656091567745208,-34.14666087457276,gps), (2019-04-18 04:03:31.0,56ed6483-d2e7-4382-a79d-93aaa1e5f7b3,-3.405296737012863,-60.96523625393486,gps), (2019-04-18 04:09:27.0,5535f25f-0b1d-40c2-a228-692363f9d9d1,-8.564119401733398,-32.26032277534485,gps), (2019-04-18 04:17:3...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array((2019-04-18 03:49:11.0,5535f25f-0b1d-40c2-a228-692363f9d9d1,-8.564107401733398,-31.26030477534485,gps), (2019-04-18 03:55:33.0,f9e65ed5-f4aa-4624-8e1f-25e3fa44b062,-22.093605798416135,-43.417351112451556,gps), (2019-04-18 03:57:52.0,2aa7fe75-4fb9-456e-b574-adab37f46996,-7.656091567745208,-34.14666087457276,gps), (2019-04-18 04:03:31.0,56ed6483-d2e7-4382-a79d-93aaa1e5f7b3,-3.405296737012863,-60.96523625393486,gps), (2019-04-18 04:09:27.0,5535f25f-0b1d-40c2-a228-692363f9d9d1,-8.564119401733398,-32.26032277534485,gps), (2019-04-18 04:17:3..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import scala.collection.JavaConversions._\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import au.com.bytecode.opencsv.CSVWriter\n",
    "import scala.util.Random\n",
    "\n",
    "println(\"Save obfuscated file to workspace\")\n",
    "val obr = rdd6.collect()\n",
    "val obr2 = obr.toList.map(e => Array(e._1.toString, e._2.toString, e._3.toString, e._4.toString, e._5.toString))\n",
    "val outputFile = new BufferedWriter(new FileWriter(\"/notebooks/obfuscate-geo-samples/obfuscated-samples.csv\"))\n",
    "val csvWriter = new CSVWriter(outputFile, ',', CSVWriter.NO_QUOTE_CHARACTER)\n",
    "val csvSchema = Array(\"time\", \"person_id\", \"lat\", \"lon\", \"type\")\n",
    "println(\"Writing CSV file\")\n",
    "csvWriter.writeNext(csvSchema)\n",
    "csvWriter.writeAll(obr2.toList)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmpPath = /tmp\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tmpPath = new Path(\"/tmp\")\n",
    "hdfs.delete(tmpPath, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Spark session\n"
     ]
    }
   ],
   "source": [
    "println(\"Stop Spark session\")\n",
    "spark.stop()\n",
    "//if you look in Spark Master UI, no application will be running after stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Toree - Scala",
   "language": "scala",
   "name": "spark_-_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
